# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LCtWo7kguiL_NeM2yjwVM5GThLE2CR0B
"""

!pip install nltk pandas

"""***Import Libraries***"""

import pandas as pd
import nltk
import string

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

"""**Importing Dataset**"""

import pandas as pd

df = pd.read_csv("/content/world news in month.csv")
df.head()

df = pd.read_csv(
    "/content/world news in month.csv",
    encoding="latin-1",
    on_bad_lines="skip"
)
df.head()

df = df[["text", "sentiment"]]
df.head()

"""**Lowercasing**

"""

df["text"] = df["text"].str.lower()

"""**Tokenization**"""

from nltk.tokenize import word_tokenize

df["tokens"] = df["text"].apply(word_tokenize)
df[["text", "tokens"]].head()

import nltk

nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize

df["tokens"] = df["text"].apply(word_tokenize)
df[["text", "tokens"]].head()

"""**Remove punctuation**"""

import string

df["tokens"] = df["tokens"].apply(
    lambda x: [w for w in x if w not in string.punctuation]
)

"""**Removing Stop-words**"""

from nltk.corpus import stopwords
nltk.download("stopwords")

stop_words = set(stopwords.words("english"))

df["tokens"] = df["tokens"].apply(
    lambda x: [w for w in x if w not in stop_words]
)
df["tokens"].head()

"""## **Stemming**"""

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

df["stemmed"] = df["tokens"].apply(
    lambda x: [stemmer.stem(w) for w in x]
)
df["stemmed"].head()

"""**Lemmatization**"""

from nltk.stem import WordNetLemmatizer
nltk.download("wordnet")

lemmatizer = WordNetLemmatizer()

df["lemmatized"] = df["tokens"].apply(
    lambda x: [lemmatizer.lemmatize(w) for w in x]
)
df["lemmatized"].head()

"""**POS Tagging**"""

nltk.download("averaged_perceptron_tagger")

df["pos_tags"] = df["tokens"].apply(nltk.pos_tag)
df["pos_tags"].head()

import nltk

nltk.download("averaged_perceptron_tagger")
nltk.download("averaged_perceptron_tagger_eng")

df["pos_tags"] = df["tokens"].apply(nltk.pos_tag)
df["pos_tags"].head()

"""**NER (Named Entity Recognition)**"""

import nltk

nltk.download('maxent_ne_chunker', download_dir='/root/nltk_data')
nltk.download('words', download_dir='/root/nltk_data')

nltk.data.path.append('/root/nltk_data')

from nltk import ne_chunk

df["ner"] = df["pos_tags"].apply(ne_chunk)
df["ner"].head()

df["ner"] = df["pos_tags"].apply(
    lambda sent: [word for word, tag in sent if tag == "NNP"]
)

df[["tokens", "pos_tags", "ner"]].head()

"""**Pre-processed Data**"""

df["clean_text"] = df["lemmatized"].apply(lambda x: " ".join(x))
df[["clean_text", "sentiment"]].head()